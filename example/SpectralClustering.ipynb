{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SpectralClustering and DBSCAN with minHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'number_of_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1b4282b0aefd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mgraphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgspan_to_eden\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0martificial_dataset\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcomplexity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mdatasetBursi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mseqs\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joachim/.local/lib/python2.7/site-packages/eden/graph.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, graphs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mfeature_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minstance_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_goodness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m             \u001b[0mfeature_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minstance_id\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/joachim/.local/lib/python2.7/site-packages/eden/graph.pyc\u001b[0m in \u001b[0;36m_test_goodness\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_test_goodness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ERROR: something went wrong, empty graph.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'number_of_nodes'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats\n",
    "import hashlib\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sparsity_factor = 0.01\n",
    "samples = 600\n",
    "features = 100\n",
    "classes = 2\n",
    "clusters_per_class = 2\n",
    "scale_ = 1.0\n",
    "threshold = -0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#code for making artificial dataset\n",
    "import random\n",
    "def random_string(length,alphabet_list):\n",
    "    rand_str = ''.join(random.choice(alphabet_list) for i in range(length))\n",
    "    return rand_str\n",
    "\n",
    "def perturb(seed,alphabet_list,p=0.5):\n",
    "    seq=''\n",
    "    for c in seed:\n",
    "        if random.random() < p: c = random.choice(alphabet_list)\n",
    "        seq += c\n",
    "    return seq\n",
    "\n",
    "def make_artificial_dataset(alphabet='ACGU', motives=None, motif_length=6, \n",
    "                            sequence_length=100, n_sequences=1000, n_motives=2, p=0.2):\n",
    "    alphabet_list=[c for c in alphabet]\n",
    "    \n",
    "    if motives is None:\n",
    "        motives=[]\n",
    "        for i in range(n_motives):\n",
    "            motives.append(random_string(motif_length,alphabet_list))\n",
    "    else:\n",
    "        motif_length = len(motives[0])\n",
    "        n_motives = len(motives)\n",
    "        \n",
    "    flanking_length = (sequence_length - motif_length ) / 2\n",
    "    n_seq_per_motif = n_sequences / n_motives\n",
    "\n",
    "    counter=0\n",
    "    seqs=[]\n",
    "    for i in range(n_seq_per_motif):\n",
    "        for j in range(n_motives):\n",
    "            left_flanking = random_string(flanking_length,alphabet_list)\n",
    "            right_flanking = random_string(flanking_length,alphabet_list)\n",
    "            noisy_motif = perturb(motives[j],alphabet_list,p)\n",
    "            seq = left_flanking + noisy_motif + right_flanking\n",
    "            seqs.append(('>ID%d'%counter,seq))\n",
    "            counter += 1\n",
    "    return motives, seqs\n",
    "\n",
    "\n",
    "motives, seqs = make_artificial_dataset()\n",
    "# print artificial_dataset\n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from eden.graph import Vectorizer\n",
    "graphs = gspan_to_eden( artificial_dataset )\n",
    "vectorizer = Vectorizer( complexity=3, nbits=20)\n",
    "datasetBursi = vectorizer.transform( seqs )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_dense, y_true = make_classification(n_samples=samples, n_features=features, n_informative=2, \n",
    "                                 n_redundant=0, n_repeated=0, n_classes=classes, \n",
    "                                 n_clusters_per_class=clusters_per_class, weights=None, flip_y=0.01,\n",
    "                                 class_sep=1.0, hypercube=True, shift=0.0, scale=scale_,\n",
    "                                 shuffle=True, random_state=1)\n",
    "print data_dense\n",
    "instances_list = []\n",
    "features_list = []\n",
    "data_list = []\n",
    "for i in xrange(len(data_dense)):\n",
    "    average_value_of_list = np.average(data_dense[i])\n",
    "    variance = np.var(data_dense[i])\n",
    "#     print \"Average: \", average_value_of_list\n",
    "#     print \"var: \", variance\n",
    "    if average_value_of_list < 0:\n",
    "        data_with_threshold = stats.threshold(data_dense[i], threshmin=average_value_of_list-variance, threshmax=average_value_of_list+variance, newval=0.0)\n",
    "    else:\n",
    "        data_with_threshold = stats.threshold(data_dense[i], threshmin=average_value_of_list-variance, threshmax=average_value_of_list+variance, newval=0.0)\n",
    "\n",
    "    #     data_with_threshold\n",
    "    for j in xrange(len(data_dense[i])):\n",
    "        instances_list.append(i)\n",
    "#         features_list.append(j)\n",
    "        hash_object = hashlib.sha256(str(j))\n",
    "        hex_dig = hash_object.hexdigest()\n",
    "        features_list.append(int(hex_dig, 16) % (features/sparsity_factor))\n",
    "#         features_list.append(hash(str(j)) % (features/sparsity_factor))\n",
    "        data_list.append(data_with_threshold[j])\n",
    "#         data_list.append(data_dense[i][j])\n",
    "#     print \"\\n\"\n",
    "data_sparse = csr_matrix((data_list, (instances_list, features_list)))\n",
    "data_sparse.eliminate_zeros()\n",
    "# print data_sparse\n",
    "print data_sparse.getnnz(1)\n",
    "# print data_with_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 0 ns, total: 128 ms\n",
      "Wall time: 130 ms\n",
      "CPU times: user 40 ms, sys: 28 ms, total: 68 ms\n",
      "Wall time: 148 ms\n",
      "Accuracy_approx:  0.012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pympler import asizeof\n",
    "neighbors = NearestNeighbors(n_jobs=4)\n",
    "neighbors.fit(data_dense)\n",
    "%time neighbors_dense = neighbors.kneighbors(n_neighbors=5, return_distance=False)\n",
    "\n",
    "neighbors_sparse = NearestNeighbors(n_jobs=4)\n",
    "neighbors_sparse.fit(data_sparse)\n",
    "%time neighbors_sparse = neighbors_sparse.kneighbors(n_neighbors=5, return_distance=False)\n",
    "\n",
    "accuracy_score_ = 0.0\n",
    "for x, y in zip(neighbors_dense, neighbors_sparse):\n",
    "    accuracy_score_ += accuracy_score(x, y)\n",
    "print \"Accuracy_approx: \", accuracy_score_ / float(len(neighbors_dense))\n",
    "# print neighbors_dense\n",
    "# print neighbors_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "Automatically created module for IPython interactive environment\n",
      "\n",
      "\n",
      "peak memory: 648.00 MiB, increment: 0.00 MiB\n",
      "CPU times: user 396 ms, sys: 44 ms, total: 440 ms\n",
      "Wall time: 571 ms\n",
      "SpectralClustering :\tAccuracy:  0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext memory_profiler\n",
    "print(__doc__)\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "from neighborsMinHash.clustering import MinHashClustering \n",
    "# from neighborsMinHash.clustering import MinHashDBSCAN\n",
    "from neighborsMinHash import MinHash\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "clustering_names = [\n",
    "    'SpectralClustering', 'MinHashSpectralClustering', 'DBSCAN', 'MinHashDBSCAN']\n",
    "\n",
    "# original algorithms\n",
    "spectral = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack',\n",
    "                                      affinity=\"nearest_neighbors\", n_neighbors=5)\n",
    "dbscan = cluster.DBSCAN(eps=.2, metric=\"euclidean\")\n",
    "\n",
    "# objects used for algorithms with precomputed minHash\n",
    "minHash0 = MinHash(n_neighbors=5)\n",
    "minHash1 = MinHash(n_neighbors=5)\n",
    "\n",
    "spectral_precomputed = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack',\n",
    "                                      affinity=\"precomputed\", n_neighbors=5)\n",
    "\n",
    "dbscanMinHash = cluster.DBSCAN(eps=.2, metric='precomputed')\n",
    "\n",
    "\n",
    "minHashClusteringSpectralClustering = MinHashClustering(minHash0, spectral_precomputed)\n",
    "minHashClusteringDBSCAN = MinHashClustering(minHash1, dbscanMinHash)\n",
    "\n",
    "clustering_algorithms=[spectral, minHashClusteringSpectralClustering, dbscan, minHashClusteringDBSCAN]\n",
    "    \n",
    "X = data_sparse\n",
    "# print \"Size of dense data: \", asizeof.asizeof(data_dense)\n",
    "# print \"Size of sparse data\", asizeof.asizeof(data_sparse)\n",
    "for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "    print \"\\n\"\n",
    "    t0 = time.time()\n",
    "    %time %memit y_pred = algorithm.fit_predict(X)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    y_pred = y_pred.astype(np.int)\n",
    "    print name, \":\\tAccuracy: \", float(\"{0:.2f}\".format(adjusted_rand_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
