{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SpectralClustering and DBSCAN with minHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext memory_profiler\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from eden.path import Vectorizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "# import everything which is needed for the clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from neighborsMinHash.clustering import MinHashClustering \n",
    "from neighborsMinHash import MinHash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code for making artificial dataset\n",
    "import random\n",
    "def random_string(length,alphabet_list):\n",
    "    rand_str = ''.join(random.choice(alphabet_list) for i in range(length))\n",
    "    return rand_str\n",
    "\n",
    "def perturb(seed,alphabet_list,p=0.5):\n",
    "    seq=''\n",
    "    for c in seed:\n",
    "        if random.random() < p: c = random.choice(alphabet_list)\n",
    "        seq += c\n",
    "    return seq\n",
    "\n",
    "def make_artificial_dataset(alphabet='ACGU', motives=None, motif_length=6, \n",
    "                            sequence_length=100, n_sequences=1000, n_motives=2, p=0.2):\n",
    "    alphabet_list=[c for c in alphabet]\n",
    "    \n",
    "    if motives is None:\n",
    "        motives=[]\n",
    "        for i in range(n_motives):\n",
    "            motives.append(random_string(motif_length,alphabet_list))\n",
    "    else:\n",
    "        motif_length = len(motives[0])\n",
    "        n_motives = len(motives)\n",
    "        \n",
    "    flanking_length = (sequence_length - motif_length ) / 2\n",
    "    n_seq_per_motif = n_sequences / n_motives\n",
    "\n",
    "    counter=0\n",
    "    seqs=[]\n",
    "    targets = []\n",
    "    for i in range(n_seq_per_motif):\n",
    "        for j in range(n_motives):\n",
    "            targets.append(j)\n",
    "            left_flanking = random_string(flanking_length,alphabet_list)\n",
    "            right_flanking = random_string(flanking_length,alphabet_list)\n",
    "            noisy_motif = perturb(motives[j],alphabet_list,p)\n",
    "            seq = left_flanking + noisy_motif + right_flanking\n",
    "            seqs.append(('>ID%d'%counter,seq))\n",
    "            counter += 1\n",
    "    return motives, seqs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setup parameters\n",
    "alphabet='ACGU'\n",
    "motif_length = 20\n",
    "motives=['A'*motif_length,'C'*motif_length,'G'*motif_length,'U'*motif_length]\n",
    "sequence_length=3 * motif_length\n",
    "n_sequences=5000\n",
    "p=0.3\n",
    "\n",
    "#make dataset\n",
    "motives_2, seqs, targets = make_artificial_dataset(alphabet=alphabet,motives=motives,\n",
    "                                        sequence_length=sequence_length,n_sequences=n_sequences,p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 248 ms, total: 19.8 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = Vectorizer(complexity=3, nbits=20)\n",
    "dataset_sparse = vectorizer.transform(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairwise_distances_ = pairwise_distances(X=dataset_sparse, metric='euclidean', n_jobs=4)\n",
    "average_value = np.average(pairwise_distances_)\n",
    "standard_deviation = np.std(pairwise_distances_)\n",
    "eps = average_value + standard_deviation\n",
    "min = np.min(pairwise_distances_)\n",
    "max = np.max(pairwise_distances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.39054454856\n",
      "1.17526904396\n",
      "0.13618964739\n"
     ]
    }
   ],
   "source": [
    "print min\n",
    "print max\n",
    "print average_value\n",
    "print standard_deviation\n",
    "eps = min + standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eps_factor = 2\n",
    "clustering_names = [\n",
    "    'SpectralClustering', 'MinHashSpectralClustering', 'DBSCAN', 'MinHashDBSCAN']\n",
    "\n",
    "# original algorithms\n",
    "spectral = SpectralClustering(n_clusters=4, eigen_solver='arpack',\n",
    "                                      affinity=\"nearest_neighbors\", n_neighbors=5)\n",
    "dbscan = DBSCAN(eps=eps*eps_factor, metric=\"euclidean\")\n",
    "\n",
    "# objects used for algorithms with precomputed minHash\n",
    "minHash0 = MinHash(n_neighbors=5, similarity=False)\n",
    "minHash1 = MinHash(n_neighbors=5, similarity=True)\n",
    "\n",
    "spectral_precomputed = SpectralClustering(n_clusters=4, eigen_solver='arpack',\n",
    "                                      affinity=\"precomputed\", n_neighbors=5)\n",
    "\n",
    "dbscan_precomputed = DBSCAN(eps=eps*eps_factor, metric='precomputed')\n",
    "\n",
    "\n",
    "minHashClusteringSpectralClustering = MinHashClustering(minHash0, spectral_precomputed)\n",
    "minHashClusteringDBSCAN = MinHashClustering(minHash1, dbscan_precomputed)\n",
    "\n",
    "clustering_algorithms=[spectral, minHashClusteringSpectralClustering, dbscan, minHashClusteringDBSCAN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "peak memory: 1213.78 MiB, increment: 597.98 MiB\n",
      "CPU times: user 17 s, sys: 628 ms, total: 17.6 s\n",
      "Wall time: 17.7 s\n",
      "SpectralClustering :\tARS:  0.7\n",
      "\n",
      "\n",
      "peak memory: 1029.41 MiB, increment: 412.04 MiB\n",
      "CPU times: user 59.7 s, sys: 520 ms, total: 1min\n",
      "Wall time: 41.7 s\n",
      "MinHashSpectralClustering :\tARS:  0.5\n",
      "\n",
      "\n",
      "peak memory: 1308.30 MiB, increment: 495.00 MiB\n",
      "CPU times: user 14.5 s, sys: 528 ms, total: 15 s\n",
      "Wall time: 15.1 s\n",
      "DBSCAN :\tARS:  0.0\n",
      "\n",
      "\n",
      "peak memory: 888.87 MiB, increment: 75.47 MiB\n",
      "CPU times: user 26.7 s, sys: 284 ms, total: 27 s\n",
      "Wall time: 8.1 s\n",
      "MinHashDBSCAN :\tARS:  0.0\n",
      "CPU times: user 1min 57s, sys: 1.96 s, total: 1min 59s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = dataset_sparse\n",
    "for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "    print \"\\n\"\n",
    "    %time %memit y_pred = algorithm.fit_predict(X)\n",
    "    y_pred = y_pred.astype(np.int)\n",
    "    print name, \":\\tARS: \", float(\"{0:.2f}\".format(adjusted_rand_score(targets, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Maximal ARS:  0  for eps:  0\n",
      "CPU times: user 2min 23s, sys: 4.49 s, total: 2min 27s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = dataset_sparse\n",
    "step = max / float(1000)\n",
    "eps = min + step\n",
    "max_ars = 0\n",
    "eps_max = 0\n",
    "for i in xrange(10):\n",
    "#     print \"\\neps: \", eps\n",
    "#     minHash1 = MinHash(n_neighbors=5, similarity=True)\n",
    "#     dbscan_precomputed = DBSCAN(eps=eps, metric='precomputed')\n",
    "#     minHashClusteringDBSCAN = MinHashClustering(minHash1, dbscan_precomputed)\n",
    "    dbscan = DBSCAN(eps=eps, metric=\"euclidean\")\n",
    "    y_pred = dbscan.fit_predict(X)\n",
    "    y_pred = y_pred.astype(np.int)\n",
    "#     print name, \":\\tARS: \", float(\"{0:.2f}\".format(adjusted_rand_score(targets, y_pred)))\n",
    "    ars = adjusted_rand_score(targets, y_pred)\n",
    "    if (ars > max_ars):\n",
    "        max_ars = ars\n",
    "        eps_max = eps\n",
    "    eps += step\n",
    "    print i\n",
    "print \"Maximal ARS: \", max_ars, \" for eps: \", eps_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
