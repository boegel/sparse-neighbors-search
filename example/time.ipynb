{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets import make_sparse_uncorrelated\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import LSHForest\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "import neighbors as kneighbors\n",
    "import random\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy.sparse import rand\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "def create_dataset(seed=None,\n",
    "                   number_of_centroids=None,\n",
    "                   number_of_instances=None,\n",
    "                   number_of_features=None,\n",
    "                   size_of_dataset=None,\n",
    "                   density=None,\n",
    "                   fraction_of_density=None\n",
    "                   ):\n",
    "    dataset_neighborhood_list = []\n",
    "    number_of_swapping_elements = int(number_of_features * density * fraction_of_density)\n",
    "    y = []\n",
    "    random_local = random.randint\n",
    "    number_of_features_density = int(number_of_features*density)-1\n",
    "    for k in xrange(number_of_centroids):\n",
    "        dataset_neighbor = rand(1, number_of_features, density=density, format='lil', random_state=seed*k)\n",
    "        nonzero_elements =  dataset_neighbor.nonzero()[1]\n",
    "        for i in xrange(size_of_dataset):\n",
    "            neighbor = dataset_neighbor.copy()\n",
    "            # random.seed(seed*k)\n",
    "            for j in xrange(number_of_swapping_elements):\n",
    "                index = random_local(0, number_of_features_density)\n",
    "                index_swap = random_local(0, number_of_features-1)\n",
    "                neighbor[0, nonzero_elements[index]] = 0\n",
    "                neighbor[0, index_swap] = 1\n",
    "            dataset_neighborhood_list.append(neighbor)\n",
    "        y.append(k)\n",
    "\n",
    "    dataset_neighborhood = vstack(dataset_neighborhood_list)\n",
    "\n",
    "    size_of_noise = number_of_instances-(number_of_centroids*size_of_dataset)\n",
    "    if size_of_noise > 0:\n",
    "            dataset_noise = rand(size_of_noise, number_of_features, format='lil', density=density, random_state=seed*seed)\n",
    "            dataset = vstack([dataset_neighborhood, dataset_noise])\n",
    "    else:\n",
    "        dataset = vstack([dataset_neighborhood])\n",
    "    random_value_generator = random.randint\n",
    "\n",
    "    # add classes for noisy data\n",
    "    for i in range(0, size_of_noise):\n",
    "        y.append(random_value_generator(0, number_of_centroids))\n",
    "\n",
    "    return csr_matrix(dataset), y\n",
    "\n",
    "def create_dataset_fixed_nonzero(seed=None,\n",
    "                   number_of_centroids=None,\n",
    "                   number_of_instances=None,\n",
    "                   number_of_features=None,\n",
    "                   size_of_dataset=None,\n",
    "                   non_zero_elements=None,\n",
    "                   fraction_of_density=None):\n",
    "    \n",
    "    if (non_zero_elements > number_of_features):\n",
    "        print \"More non-zero elements than features!\"\n",
    "        return\n",
    "    density = non_zero_elements / float(number_of_features)\n",
    "    print \"Desity:\" , density\n",
    "    dataset_neighborhood_list = []\n",
    "    number_of_swapping_elements = int(non_zero_elements * fraction_of_density)\n",
    "    y = []\n",
    "    random_local = random.randint\n",
    "    \n",
    "    for k in xrange(number_of_centroids):\n",
    "        dataset_neighbor = rand(1, number_of_features, density=density, format='lil', random_state=seed*k)\n",
    "        nonzero_elements =  dataset_neighbor.nonzero()[1]\n",
    "        for i in xrange(size_of_dataset):\n",
    "            neighbor = dataset_neighbor.copy()\n",
    "            # random.seed(seed*k)\n",
    "            for j in xrange(number_of_swapping_elements):\n",
    "                index = random_local(0, non_zero_elements-1)\n",
    "                index_swap = random_local(0, number_of_features-1)\n",
    "                neighbor[0, nonzero_elements[index]] = 0\n",
    "                neighbor[0, index_swap] = 1\n",
    "            dataset_neighborhood_list.append(neighbor)\n",
    "        y.append(k)\n",
    "\n",
    "    dataset_neighborhood = vstack(dataset_neighborhood_list)\n",
    "\n",
    "    size_of_noise = number_of_instances-(number_of_centroids*size_of_dataset)\n",
    "    if size_of_noise > 0:\n",
    "            dataset_noise = rand(size_of_noise, number_of_features, format='lil', density=density, random_state=seed*seed)\n",
    "            dataset = vstack([dataset_neighborhood, dataset_noise])\n",
    "    else:\n",
    "        dataset = vstack([dataset_neighborhood])\n",
    "    random_value_generator = random.randint\n",
    "\n",
    "    # add classes for noisy data\n",
    "    for i in range(0, size_of_noise):\n",
    "        y.append(random_value_generator(0, number_of_centroids))\n",
    "\n",
    "    return csr_matrix(dataset), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def measure_peformance( dataset, y, n_neighbors_sklearn = 5, n_neighbors_minHash = 5, number_of_instances = [1000, 10000],\n",
    "    number_of_features = [10000,  100000], allNeighbors=False):\n",
    "#     n_neighbors_sklearn = 5\n",
    "#     n_neighbors_minHash = 5\n",
    "\n",
    "    \n",
    "    time_fit_sklearn = []\n",
    "    time_fit_minHash = []\n",
    "    time_fit_lshf = []\n",
    "    time_query_time_50_1_sklearn = []\n",
    "    time_query_time_50_1_minHash_exact = []\n",
    "    time_query_time_50_1_minHash_approx = []\n",
    "    time_query_time_50_1_lshf = []\n",
    "\n",
    "    time_query_time_1_50_sklearn = []\n",
    "    time_query_time_1_50_minHash_exact = []\n",
    "    time_query_time_1_50_minHash_approx = []\n",
    "    time_query_time_1_50_lshf = []\n",
    "    \n",
    "    accuracy_1_50_lshf = []\n",
    "    accuracy_1_50_minHash_exact = []\n",
    "    accuracy_1_50_minHash_aprox = []\n",
    "    \n",
    "    \n",
    "#     seed = 6\n",
    "    centroids = 8\n",
    "    size_of_datasets = 7\n",
    "#     number_of_instances = [1000, 10000]\n",
    "#     number_of_features = [10000,  100000]\n",
    "#     density = 0.01\n",
    "#     fraction_of_density = 0.2\n",
    "\n",
    "    size_of_query = 50\n",
    "    for dataset_ in dataset:\n",
    "        \n",
    "        nearest_neighbor_sklearn = NearestNeighbors(n_neighbors = n_neighbors_sklearn)\n",
    "        nearest_neighbor_minHash = kneighbors.MinHashNearestNeighbors(n_neighbors = n_neighbors_minHash)\n",
    "        nearest_neighbor_lshf = LSHForest(n_estimators=20, n_candidates=200, n_neighbors=7)\n",
    "        \n",
    "        time_start = time.time()\n",
    "        nearest_neighbor_sklearn.fit(dataset_)\n",
    "        time_end = time.time()\n",
    "        time_fit_sklearn.append(time_end - time_start)\n",
    "\n",
    "        time_start = time.time()\n",
    "        nearest_neighbor_minHash.fit(dataset_)\n",
    "        time_end = time.time()\n",
    "        time_fit_minHash.append(time_end - time_start)\n",
    "\n",
    "        time_start = time.time()\n",
    "        nearest_neighbor_lshf.fit(dataset_)\n",
    "        time_end = time.time()\n",
    "        time_fit_lshf.append(time_end - time_start)\n",
    "        \n",
    "        if allNeighbors:\n",
    "            query_ids = []\n",
    "            for i in range(size_of_query):\n",
    "                query_ids.append(random.randint(0, centroids * size_of_datasets))\n",
    "            print \"Query ids: \", query_ids\n",
    "    #         print \"Dataset: \", dataset_\n",
    "            print \"Shape: \", dataset_.shape\n",
    "            query = dataset_[query_ids]\n",
    "        else:\n",
    "            query = None\n",
    "\n",
    "        time_start = time.time()\n",
    "        n_neighbors_sklearn_1_50 = nearest_neighbor_sklearn.kneighbors(query, return_distance=False)\n",
    "        time_end = time.time()\n",
    "        time_query_time_1_50_sklearn.append(time_end - time_start)\n",
    "\n",
    "        time_start = time.time()\n",
    "        n_neighbors_minHash_exact_1_50 = nearest_neighbor_minHash.kneighbors(query, algorithm=\"exact\", return_distance=False)\n",
    "        time_end = time.time()\n",
    "        time_query_time_1_50_minHash_exact.append(time_end - time_start)\n",
    "        \n",
    "\n",
    "        time_start = time.time()\n",
    "        n_neighbors_minHash_approx_1_50 = nearest_neighbor_minHash.kneighbors(query, algorithm=\"approximate\",return_distance=False)\n",
    "        time_end = time.time()\n",
    "        time_query_time_1_50_minHash_approx.append(time_end - time_start)\n",
    "\n",
    "        time_start = time.time()\n",
    "        n_neighbors_lshf_1_50 = nearest_neighbor_lshf.kneighbors(query,return_distance=False)\n",
    "        time_end = time.time()\n",
    "        time_query_time_1_50_lshf.append(time_end - time_start)\n",
    "\n",
    "        \n",
    "        accuracy_1_50_lshf.append(np.in1d(n_neighbors_lshf_1_50, n_neighbors_sklearn_1_50).mean())\n",
    "        accuracy_1_50_minHash_exact.append(np.in1d(n_neighbors_minHash_exact_1_50, n_neighbors_sklearn_1_50).mean())\n",
    "        accuracy_1_50_minHash_aprox.append(np.in1d(n_neighbors_minHash_approx_1_50, n_neighbors_sklearn_1_50).mean())\n",
    "    \n",
    "    \n",
    "        time_query_time_50_1_sklearn_loc = []\n",
    "        time_query_time_50_1_sklearn_loc = []\n",
    "        for i in range(size_of_query):\n",
    "            time_start = time.time()\n",
    "            nearest_neighbor_sklearn.kneighbors(query[i],return_distance=False)\n",
    "            time_end = time.time()\n",
    "            time_query_time_50_1_sklearn_loc.append(time_end - time_start)\n",
    "        time_query_time_50_1_sklearn.append(np.sum(time_query_time_50_1_sklearn_loc))\n",
    "        time_query_time_50_1_minHash_exact_loc = []\n",
    "        for i in range(size_of_query):\n",
    "            time_start = time.time()\n",
    "            nearest_neighbor_minHash.kneighbors(query[i], algorithm=\"exact\",return_distance=False)\n",
    "            time_end = time.time()\n",
    "            time_query_time_50_1_minHash_exact_loc.append(time_end - time_start)\n",
    "        time_query_time_50_1_minHash_exact.append(np.sum(time_query_time_50_1_minHash_exact_loc))\n",
    "        time_query_time_50_1_minHash_approx_loc = []\n",
    "        for i in range(size_of_query):\n",
    "            time_start = time.time()\n",
    "            nearest_neighbor_minHash.kneighbors(query[i], algorithm=\"approximate\",return_distance=False)\n",
    "            time_end = time.time()\n",
    "            time_query_time_50_1_minHash_approx_loc.append(time_end - time_start)\n",
    "        time_query_time_50_1_minHash_approx.append(np.sum(time_query_time_50_1_minHash_approx_loc))\n",
    "        time_query_time_50_1_lshf_loc = []\n",
    "        for i in range(size_of_query):\n",
    "            time_start = time.time()\n",
    "            nearest_neighbor_lshf.kneighbors(query[i], return_distance=False)\n",
    "            time_end = time.time()\n",
    "            time_query_time_50_1_lshf_loc.append(time_end - time_start)\n",
    "        time_query_time_50_1_lshf.append(np.sum(time_query_time_50_1_lshf_loc))\n",
    "\n",
    "    # for i in xrange(len(time_fit_sklearn)):\n",
    "\n",
    "    #     print \"(50_1) Number of queries to be better than sklearn: \", (time_fit_minHash[i])/ (time_query_time_50_1_sklearn[i] -time_query_time_50_1_minHash_exact[i])\n",
    "    #     print \"(1_50) Number of queries to be better than sklearn: \", (time_fit_minHash[i]) /(time_query_time_1_50_sklearn[i] -time_query_time_1_50_minHash_exact[i])\n",
    "    plt.figure(figsize=(30,11))\n",
    "    N = len(number_of_instances) * len(number_of_features)\n",
    "\n",
    "    ind = np.arange(N)    # the x locations for the groups\n",
    "    bar_width = 0.05       # the width of the bars: can also be len(x) sequence\n",
    "    #\"r\", \"b\", \"g\", \"c\", \"m\", \"y\", \"k\", \"w\"\n",
    "    p1 = plt.bar(ind, time_fit_sklearn,   bar_width, color='r', label = \"fit_sklearn\")\n",
    "    p2 = plt.bar(ind + bar_width, time_fit_minHash,   bar_width, color='g', label=\"fit_minHash\")\n",
    "    p10 = plt.bar(ind + 2*bar_width, time_fit_lshf,   bar_width, color='w', label=\"fit_lshf\")\n",
    "    p3 = plt.bar(ind + 3*bar_width, time_query_time_50_1_sklearn,   bar_width, color='r', label=\"query_sklearn_50_1\")\n",
    "    p4 = plt.bar(ind + 4*bar_width, time_query_time_50_1_minHash_exact,   bar_width, color='b', label=\"query_minHash_exact_50_1\")\n",
    "    p5 = plt.bar(ind + 5*bar_width, time_query_time_50_1_minHash_approx,   bar_width, color='m',label=\"query_minHash_approx_50_1\")\n",
    "    p11 = plt.bar(ind + 6*bar_width, time_query_time_50_1_lshf,   bar_width, color='w',label=\"query_lshf_50_1\")\n",
    "    p6 = plt.bar(ind + 7*bar_width, time_query_time_1_50_sklearn,   bar_width, color='r',label=\"query_sklearn_1_50\")\n",
    "    p7 = plt.bar(ind + 8*bar_width, time_query_time_1_50_minHash_exact,   bar_width, color='b',label=\"query_minHash_exact_1_50\")\n",
    "    p8 = plt.bar(ind + 9*bar_width, time_query_time_1_50_minHash_approx,   bar_width, color='m',label=\"query_minHash_approx_1_50\")\n",
    "    p9 = plt.bar(ind + 10*bar_width, time_query_time_1_50_lshf,   bar_width, color='w',label=\"query_lshf_1_50\")\n",
    "\n",
    "\n",
    "    plt.ylabel('Time')\n",
    "    title = 'Time for fitting and query for different datasets; density: '\n",
    "    title += str(density)\n",
    "    plt.title(title)\n",
    "    plt.xticks(ind + bar_width, (\"1000 / 10000\", \"1000 / 100000\", \"10000 / 10000\",\"10000 / 100000\"))\n",
    "    plt.yticks(np.arange(0,11,0.5))\n",
    "\n",
    "    # plt.legend( (p1[0], p2[0], p3[0], p4[0], p5[0], p6[0], p7[0], p8[0]), \n",
    "    #            ('fit_sklearn', 'fit_minHash', 'query_sklearn_50_1','query_minHash_exact_50_1',\n",
    "    #                              'query_minHash_50_1',\n",
    "    #                             'query_sklearn_1_50','query_minHash_exact_1_50','query_minHash_approx_1_50') )\n",
    "    plt.legend(loc='upper left', fontsize='small')\n",
    "    plt.show()\n",
    "    \n",
    "    print \"Accuracy lshf:\", np.mean(accuracy_1_50_lshf)\n",
    "    print \"Accuracy minHash_exact:\", np.mean(accuracy_1_50_minHash_exact)\n",
    "    print \"Accuracy minHash_appox:\", np.mean(accuracy_1_50_minHash_aprox)\n",
    "    for i in xrange(len(time_fit_sklearn)):\n",
    "        print \"Fitting time sklearn: \", time_fit_sklearn[i]\n",
    "        print \"Fitting time minHash: \", time_fit_minHash[i]\n",
    "        print \"Fitting time lshf: \", time_fit_lshf[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 6\n",
    "centroids = 8\n",
    "size_of_datasets = 7\n",
    "number_of_instances = [1000]\n",
    "number_of_features = [10000]\n",
    "density = 0.01\n",
    "fraction_of_density = 0.2\n",
    "n_neighbors_sklearn = 5\n",
    "n_neighbors_minHash = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-426efc3f26d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdataset_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmeasure_peformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors_minHash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-fcf4ad42fc69>\u001b[0m in \u001b[0;36mmeasure_peformance\u001b[0;34m(dataset, y, n_neighbors_sklearn, n_neighbors_minHash, number_of_instances, number_of_features, allNeighbors)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mn_neighbors_lshf_1_50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearest_neighbor_lshf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtime_query_time_1_50_lshf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/neighbors/approximate.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[1;32m    350\u001b[0m                              array.ndim)\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     50\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     51\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 52\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_array = []\n",
    "for instances in number_of_instances:\n",
    "    for features in number_of_features:\n",
    "        dataset, y = create_dataset(seed=seed, number_of_centroids=centroids, number_of_instances=instances,\n",
    "                                         number_of_features=features,density = density,\n",
    "                                         fraction_of_density=fraction_of_density, size_of_dataset = size_of_datasets)\n",
    "        dataset_array.append(dataset)\n",
    "\n",
    "measure_peformance(dataset_array, y, n_neighbors_sklearn, n_neighbors_minHash, number_of_instances, number_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Desity: 0.01\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-71fe7283c546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                    fraction_of_density=0.2)\n\u001b[1;32m     13\u001b[0m         \u001b[0mdataset_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_fixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmeasure_peformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors_minHash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-fcf4ad42fc69>\u001b[0m in \u001b[0;36mmeasure_peformance\u001b[0;34m(dataset, y, n_neighbors_sklearn, n_neighbors_minHash, number_of_instances, number_of_features, allNeighbors)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mn_neighbors_lshf_1_50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearest_neighbor_lshf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtime_query_time_1_50_lshf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/neighbors/approximate.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[1;32m    350\u001b[0m                              array.ndim)\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     50\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     51\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 52\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "number_of_instances = [100]\n",
    "number_of_features = [int(1e3)]#, int(1e4), int(1e5)]\n",
    "dataset_array = []\n",
    "for instances in number_of_instances:\n",
    "    for features in number_of_features:\n",
    "        dataset_fixed, y = create_dataset_fixed_nonzero(seed=5,\n",
    "                   number_of_centroids=7,\n",
    "                   number_of_instances=instances,\n",
    "                   number_of_features=features,\n",
    "                   size_of_dataset=8,\n",
    "                   non_zero_elements=10,\n",
    "                   fraction_of_density=0.2)\n",
    "        dataset_array.append(dataset_fixed)\n",
    "measure_peformance(dataset_array, y, n_neighbors_sklearn, n_neighbors_minHash, number_of_instances, number_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named eden.converter.graph.gspan",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-45d98f903a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgspan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgspan_to_eden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgspan_to_eden\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'http://www.bioinf.uni-freiburg.de/~costa/bursi.gspan'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgraphs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named eden.converter.graph.gspan"
     ]
    }
   ],
   "source": [
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from eden.graph import Vectorizer\n",
    "graphs = gspan_to_eden( 'http://www.bioinf.uni-freiburg.de/~costa/bursi.gspan' )\n",
    "vectorizer = Vectorizer( r=2,d=5 )\n",
    "X = vectorizer.transform( graphs )\n",
    "y = None\n",
    "print \"Shape\", X.shape\n",
    "print \"nnz:\", X.getnnz()\n",
    "print \"sparsity: \", X.getnnz() / float(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NearestNeighbors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bba724c71b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnearest_neighbor_sklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_neighbors_sklearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnearest_neighbor_minHash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinHashNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_neighbors_minHash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnearest_neighbor_lshf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSHForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_candidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_neighbors_minHash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NearestNeighbors' is not defined"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_sklearn = NearestNeighbors(n_neighbors = n_neighbors_sklearn)\n",
    "nearest_neighbor_minHash = kneighbors.MinHashNearestNeighbors(n_neighbors = n_neighbors_minHash)\n",
    "nearest_neighbor_lshf = LSHForest(n_estimators=20, n_candidates=200, n_neighbors=n_neighbors_minHash)\n",
    "\n",
    "time_start = time.time()\n",
    "nearest_neighbor_sklearn.fit(X)\n",
    "time_end = time.time()\n",
    "time_fit_sklearn = time_end - time_start\n",
    "\n",
    "time_start = time.time()\n",
    "nearest_neighbor_minHash.fit(X)\n",
    "# print nearest_neighbor_minHash._inverseIndex._inverse_index\n",
    "time_end = time.time()\n",
    "time_fit_minHash = time_end - time_start\n",
    "\n",
    "time_start = time.time()\n",
    "nearest_neighbor_lshf.fit(X)\n",
    "time_end = time.time()\n",
    "time_fit_lshf = time_end - time_start\n",
    "\n",
    "time_start = time.time()\n",
    "n_neighbors_sklearn_1_50 = nearest_neighbor_sklearn.kneighbors(return_distance=False)\n",
    "time_end = time.time()\n",
    "time_query_time_1_50_sklearn = time_end - time_start\n",
    "\n",
    "time_start = time.time()\n",
    "n_neighbors_minHash_exact_1_50 = nearest_neighbor_minHash.kneighbors(algorithm=\"exact\", return_distance=False)\n",
    "time_end = time.time()\n",
    "time_query_time_1_50_minHash_exact = time_end - time_start\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "n_neighbors_minHash_approx_1_50 = nearest_neighbor_minHash.kneighbors(algorithm=\"approximate\",return_distance=False)\n",
    "# print n_neighbors_minHash_approx_1_50\n",
    "time_end = time.time()\n",
    "time_query_time_1_50_minHash_approx = time_end - time_start\n",
    "\n",
    "time_start = time.time()\n",
    "n_neighbors_lshf_1_50 = nearest_neighbor_lshf.kneighbors(X,return_distance=False)\n",
    "time_end = time.time()\n",
    "time_query_time_1_50_lshf = time_end - time_start\n",
    "\n",
    "\n",
    "accuracy_1_50_lshf = np.in1d(n_neighbors_lshf_1_50, n_neighbors_sklearn_1_50).mean()\n",
    "accuracy_1_50_minHash_exact = np.in1d(n_neighbors_minHash_exact_1_50, n_neighbors_sklearn_1_50).mean()\n",
    "accuracy_1_50_minHash_approx = np.in1d(n_neighbors_minHash_approx_1_50, n_neighbors_sklearn_1_50).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,5))\n",
    "N = 1\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "bar_width = 0.05       # the width of the bars: can also be len(x) sequence\n",
    "#\"r\", \"b\", \"g\", \"c\", \"m\", \"y\", \"k\", \"w\"\n",
    "p1 = plt.bar(ind, time_fit_sklearn,   bar_width, color='r', label = \"fit_sklearn\")\n",
    "p2 = plt.bar(ind + bar_width, time_fit_minHash,   bar_width, color='g', label=\"fit_minHash\")\n",
    "p3 = plt.bar(ind + 2*bar_width, time_fit_lshf,   bar_width, color='w', label=\"fit_lshf\")\n",
    "\n",
    "p4 = plt.bar(ind + 3*bar_width, time_query_time_1_50_sklearn,   bar_width, color='r',label=\"query_sklearn\")\n",
    "p5 = plt.bar(ind + 4*bar_width, time_query_time_1_50_minHash_exact,   bar_width, color='g',label=\"query_minHash_exact\")\n",
    "p6 = plt.bar(ind + 5*bar_width, time_query_time_1_50_minHash_approx,   bar_width, color='g',label=\"query_minHash_approx\")\n",
    "p7 = plt.bar(ind + 6*bar_width, time_query_time_1_50_lshf,   bar_width, color='w',label=\"query_lsh\")\n",
    "\n",
    "\n",
    "plt.ylabel('Time')\n",
    "title = 'bursi.gspan dataset'\n",
    "plt.title(title)\n",
    "plt.xticks(ind + bar_width, ())\n",
    "# plt.yticks(np.arange(0,10,1))\n",
    "\n",
    "# plt.legend( (p1[0], p2[0], p3[0], p4[0], p5[0], p6[0], p7[0], p8[0]), \n",
    "#            ('fit_sklearn', 'fit_minHash', 'query_sklearn_50_1','query_minHash_exact_50_1',\n",
    "#                              'query_minHash_50_1',\n",
    "#                             'query_sklearn_1_50','query_minHash_exact_1_50','query_minHash_approx_1_50') )\n",
    "plt.legend(loc='upper left', fontsize='small')\n",
    "plt.show()\n",
    "\n",
    "print \"Accuracy lshf: \", accuracy_1_50_lshf\n",
    "print \"Accuracy minHash_exact: \", accuracy_1_50_minHash_exact\n",
    "print \"Accuracy minHash_approx: \", accuracy_1_50_minHash_approx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
